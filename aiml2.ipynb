{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport os\nprint(os.listdir(\"../input/fruits-360_dataset/fruits-360\"))\n\n","execution_count":1,"outputs":[{"output_type":"stream","text":"['test-multiple_fruits', 'Training', 'readme.md', 'LICENSE', 'papers', 'Test']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import TensorDataset,DataLoader\n\nchannel_means = (0.485, 0.456, 0.406)\nchannel_stds = (0.229, 0.224, 0.225)\ntransformation = transforms.Compose([\n        transforms.Resize(size=(150,150)),\n        transforms.ToTensor(),\n        transforms.Normalize(channel_means,channel_stds )])\n\nbatch = 64\npath = \"../input/fruits-360_dataset/fruits-360/\"\ntrain_dataset = torchvision.datasets.ImageFolder(path+\"Training\", transform=transformation)\ntrain_loader = DataLoader(train_dataset,batch_size=batch, shuffle=True)\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_val = torchvision.datasets.ImageFolder(path + \"Test\", transform=transformation)\ntest = int(len(test_val)/2)\nval = int(len(test_val) - test)\nval_dataset, test_dataset = torch.utils.data.random_split(test_val, [val, test])\n\nval_loader = DataLoader(val_dataset,batch_size=batch, shuffle=True)\ntest_loader = DataLoader(test_dataset,batch_size=batch, shuffle=True)\n\nprint(len(train_dataset),len(val_dataset),len(test_dataset))","execution_count":8,"outputs":[{"output_type":"stream","text":"53177 8923 8922\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport math\nclass NN(nn.Module):\n    def __init__(self):\n        super(NN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=50, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(3),\n            nn.Conv2d(in_channels=50, out_channels=64, kernel_size=4),\n            nn.ReLU(),\n            nn.MaxPool2d(4),\n            nn.Conv2d(in_channels=64, out_channels=80, kernel_size=2),\n            nn.ReLU(),\n            nn.MaxPool2d(5))\n        \n            \n        self.layer2 = nn.Sequential(\n            nn.Linear(320,128), nn.ReLU(),\n            nn.Linear(128,103)\n        )\n    def forward(self, x):\n        y = self.layer2(self.layer1(x).view(x.size(0), -1))\n        return y","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(model, train_dl,val_dl, lr, epoches,tolerance):\n    model.cuda()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    best_loss = 100\n    cur_tol = tolerance\n    for epoche in range(epoches):\n        ep_loss = 0\n        for xx,yy in train_dl:\n            xx,yy = xx.cuda(), yy.cuda()\n            optimizer.zero_grad()\n            y_pred = model(xx)\n            loss = criterion(y_pred, yy)\n            loss.backward()\n            ep_loss+=loss.item()\n            optimizer.step()\n        print(\"Loss: {}\".format(ep_loss/len(train_dl)))\n        with torch.no_grad():\n            val_loss=0\n            for xx,yy in val_dl:\n                xx,yy = xx.cuda(), yy.cuda()\n                y_pred = model(xx)\n                loss = criterion(y_pred, yy)\n                val_loss+=loss.item()\n            val_loss/=len(val_dl)\n            if best_loss>= val_loss:\n                best_loss = val_loss\n                cur_tol = tolerance\n                torch.save(model.state_dict(), \"..\\bestmodel.mod\")\n            else:\n                cur_tol -= 1\n            if cur_tol==0:\n                model.load_state_dict(torch.load(\"..\\bestmodel.mod\"))\n                break\n        print(\"---->Val loss: {}\".format(val_loss))\n    print(\"Stop train.\")\n    model.cpu()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = NN()\nfit(net,train_loader,val_loader,0.005,6,3)","execution_count":11,"outputs":[{"output_type":"stream","text":"Loss: 0.6618138063994878\n---->Val loss: 0.5341633870665516\nLoss: 0.14011340851925772\n---->Val loss: 0.25222966372966765\nLoss: 0.10574461072324702\n---->Val loss: 0.4710174143580454\nLoss: 0.12019159664006547\n---->Val loss: 0.32646183451371535\nLoss: 0.08818072749173748\nStop train.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"net.load_state_dict(torch.load(\"..\\bestmodel.mod\"))\ny_true = []\ny_pred = []\nfor xx,yy in test_loader:\n    net.cuda()\n    xx,yy = xx.cuda(), yy.cuda()\n    out = net(xx).argmax(dim=1)\n    y_true.extend(yy.tolist())\n    y_pred.extend(out.tolist())\nnet.cpu()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"NN(\n  (layer1): Sequential(\n    (0): Conv2d(3, 50, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(50, 64, kernel_size=(4, 4), stride=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(64, 80, kernel_size=(2, 2), stride=(1, 1))\n    (7): ReLU()\n    (8): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer2): Sequential(\n    (0): Linear(in_features=320, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=103, bias=True)\n  )\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_true, y_pred))\nprint(confusion_matrix(y_true, y_pred))","execution_count":13,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.45      0.83      0.59        81\n           1       0.93      0.84      0.88        79\n           2       0.93      0.80      0.86        83\n           3       1.00      0.94      0.97        81\n           4       1.00      1.00      1.00        84\n           5       1.00      1.00      1.00        80\n           6       0.86      0.96      0.91        84\n           7       0.73      0.73      0.73        90\n           8       1.00      0.46      0.63        79\n           9       1.00      0.73      0.85        79\n          10       0.99      1.00      0.99        83\n          11       1.00      0.77      0.87        88\n          12       1.00      1.00      1.00       115\n          13       1.00      1.00      1.00        97\n          14       1.00      1.00      1.00        68\n          15       0.89      1.00      0.94        74\n          16       1.00      1.00      1.00        83\n          17       1.00      0.88      0.93        81\n          18       0.96      0.95      0.95        76\n          19       1.00      1.00      1.00        84\n          20       1.00      1.00      1.00        71\n          21       0.99      1.00      0.99        86\n          22       0.88      1.00      0.94        82\n          23       0.54      1.00      0.70        72\n          24       1.00      0.98      0.99       110\n          25       1.00      0.99      1.00       122\n          26       1.00      0.92      0.96        79\n          27       1.00      1.00      1.00        84\n          28       1.00      1.00      1.00        85\n          29       0.79      0.87      0.83        63\n          30       0.86      0.88      0.87        78\n          31       1.00      0.97      0.98        90\n          32       0.80      1.00      0.89        88\n          33       0.84      1.00      0.91        92\n          34       0.95      1.00      0.97       163\n          35       0.90      0.94      0.92        80\n          36       1.00      1.00      1.00        86\n          37       1.00      1.00      1.00        83\n          38       1.00      1.00      1.00        87\n          39       1.00      1.00      1.00        84\n          40       1.00      1.00      1.00        80\n          41       0.94      1.00      0.97        80\n          42       1.00      1.00      1.00        83\n          43       1.00      1.00      1.00        77\n          44       1.00      1.00      1.00        86\n          45       0.90      1.00      0.95        83\n          46       1.00      1.00      1.00        76\n          47       1.00      1.00      1.00        70\n          48       1.00      0.91      0.95        91\n          49       1.00      1.00      1.00        76\n          50       1.00      1.00      1.00        92\n          51       1.00      1.00      1.00        72\n          52       1.00      1.00      1.00        82\n          53       1.00      1.00      1.00        80\n          54       1.00      1.00      1.00        87\n          55       0.88      0.76      0.81        58\n          56       1.00      1.00      1.00        77\n          57       1.00      1.00      1.00       118\n          58       0.91      1.00      0.95        82\n          59       0.62      0.93      0.74        86\n          60       1.00      1.00      1.00        76\n          61       1.00      1.00      1.00        87\n          62       0.88      1.00      0.94        83\n          63       0.73      0.53      0.62        88\n          64       1.00      1.00      1.00       122\n          65       1.00      0.61      0.76        85\n          66       1.00      1.00      1.00        81\n          67       1.00      0.86      0.92        77\n          68       1.00      0.67      0.80        42\n          69       1.00      1.00      1.00        79\n          70       1.00      0.89      0.94       105\n          71       0.81      1.00      0.89        93\n          72       1.00      0.89      0.94        79\n          73       1.00      1.00      1.00        76\n          74       1.00      0.98      0.99       105\n          75       1.00      1.00      1.00       110\n          76       1.00      1.00      1.00        82\n          77       1.00      1.00      1.00        79\n          78       1.00      0.95      0.97        78\n          79       1.00      0.98      0.99        82\n          80       0.56      1.00      0.72        87\n          81       1.00      0.45      0.62        71\n          82       1.00      1.00      1.00        70\n          83       0.99      0.98      0.99       146\n          84       1.00      0.49      0.66        88\n          85       0.86      1.00      0.92        67\n          86       1.00      1.00      1.00        90\n          87       0.99      0.84      0.91        87\n          88       1.00      1.00      1.00        85\n          89       1.00      1.00      1.00        82\n          90       1.00      0.90      0.95        82\n          91       0.99      0.98      0.98        82\n          92       1.00      0.69      0.82       117\n          93       1.00      0.94      0.97        80\n          94       1.00      1.00      1.00        75\n          95       1.00      0.85      0.92       124\n          96       1.00      1.00      1.00       105\n          97       0.86      0.99      0.92       125\n          98       0.97      1.00      0.98        92\n          99       0.90      1.00      0.95        82\n         100       1.00      0.96      0.98        72\n         101       1.00      1.00      1.00        81\n         102       1.00      1.00      1.00       133\n\n   micro avg       0.94      0.94      0.94      8922\n   macro avg       0.95      0.94      0.94      8922\nweighted avg       0.95      0.94      0.94      8922\n\n[[ 67   0   0 ...   0   0   0]\n [  0  66   0 ...   0   0   0]\n [  0   0  66 ...   0   0   0]\n ...\n [  0   0   0 ...  69   0   0]\n [  0   0   0 ...   0  81   0]\n [  0   0   0 ...   0   0 133]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}